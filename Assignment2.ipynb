{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import pmdarima as pm\n",
    "from pmdarima import pipeline, arima, model_selection\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provided Code to Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "price_path = os.path.join(os.getcwd(),'elspotprices_19to21.csv')\n",
    "df_prices = pd.read_csv(price_path)\n",
    "df_prices['HourUTC'] = pd.to_datetime(df_prices[\"HourUTC\"])\n",
    "# Remove timezone information\n",
    "df_prices[\"HourUTC\"] = df_prices[\"HourUTC\"].dt.tz_localize(None)\n",
    "# Sort values\n",
    "df_prices = df_prices.sort_values('HourUTC')\n",
    "df_prices = df_prices.reset_index(drop=True)\n",
    "# Load the data\n",
    "Exog_path = os.path.join(os.getcwd(),'production_19to21.csv')\n",
    "df_exog = pd.read_csv(Exog_path)\n",
    "df_exog['HourUTC'] = pd.to_datetime(df_exog[\"HourUTC\"])\n",
    "# Sort values\n",
    "df_exog = df_exog.sort_values('HourUTC')\n",
    "df_exog = df_exog.reset_index(drop=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing Dataset\n",
    "Remove HourDK, SpotPriceEUR, and choose DK1 as the region of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prices = df_prices.loc[df_prices[\"PriceArea\"] == 'DK1']\n",
    "df_prices = df_prices.drop(columns=['HourDK','SpotPriceEUR'])\n",
    "df_prices = df_prices.reset_index(drop=True)\n",
    "\n",
    "df_exog = df_exog.loc[df_exog[\"PriceArea\"] == 'DK1']\n",
    "df_exog = df_exog.drop(columns=['HourDK'])\n",
    "df_exog = df_exog.reset_index(drop=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce df_prices & df_exog size to only have only data from the date specified onwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_train_date = '2021-08-30T23:00:00.000'\n",
    "\n",
    "df_prices_start_date = np.min(df_prices.index[df_prices['HourUTC'] == start_train_date])\n",
    "df_prices = df_prices[df_prices_start_date:]\n",
    "df_prices = df_prices.reset_index(drop=True)\n",
    "\n",
    "df_exog_start_date = np.min(df_exog.index[df_exog['HourUTC'] == start_train_date])\n",
    "df_exog = df_exog[df_exog_start_date:]\n",
    "df_exog = df_exog.reset_index(drop=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and Testing Datasets\n",
    "Seperate test data and training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#get index of the given end training set date\n",
    "end_training_set_date = '2021-12-01T00:00:00.000'\n",
    "set_train_size = np.max(df_prices.index[df_prices['HourUTC'] == end_training_set_date])\n",
    "\n",
    "train, test = model_selection.train_test_split(df_prices, train_size=set_train_size)\n",
    "X_train, X_test = model_selection.train_test_split(df_exog, train_size=set_train_size)\n",
    "\n",
    "train = train[\"SpotPriceDKK\"].values\n",
    "test = test[\"SpotPriceDKK\"].values\n",
    "data = df_prices['SpotPriceDKK'].values\n",
    "\n",
    "n_train = len(train)\n",
    "n_test = len(test)\n",
    "n_data = len(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "Develop an ARIMA model to predict electricity prices. Your goal in all following tasks is to\n",
    "achieve the best possible performance. In all sub-tasks, report the RMSE values you achieve\n",
    "with your models. Pick a zone (DK1 or DK2) of your liking"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.1\n",
    "Use NO exogenous variables in your model and make hour-ahead and day-ahead prediction\n",
    "for your testing dataset. You can use a seasonal ARIMA or FourierFeaturizer and any data\n",
    "transformation you want in your model, but no features from the production 19to21 dataset.\n",
    "Establish a suitable persistence forecast, and report the RMSE values in all 4 cases (your model\n",
    "and persistence, and for both hour-ahead and day-ahead). Briefly discuss your results and the\n",
    "difference between the two forecasting cases."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hour Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Persistence Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_length = 1\n",
    "\n",
    "Persistence_hour = []\n",
    "\n",
    "time_estimate_frames = int(len(test)/time_length)\n",
    "\n",
    "for i in range(time_estimate_frames):\n",
    "\n",
    "    spliced_estiamte_frame = data[len(train)+i*time_length-time_length:len(train)+(i+1)*time_length-time_length]\n",
    "    Persistence_hour.extend(spliced_estiamte_frame)\n",
    "\n",
    "RMSE_P_H = mean_squared_error(Persistence_hour, test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ARIMA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_length = 1\n",
    "\n",
    "ARIMA_Forecast_H = []\n",
    "\n",
    "time_estimate_frames = int(len(test)/time_length)\n",
    "\n",
    "#m is set to 24 due to the seasonal affect of the 24 hour energy cycle\n",
    "ARIMA = pm.auto_arima(train, trace = True, seasonal=True, iter = 100)\n",
    "\n",
    "for i in tqdm(range(time_estimate_frames)):\n",
    "    time_step_forcast = ARIMA.predict(n_periods = time_length)\n",
    "\n",
    "    ARIMA_Forecast_H.extend(time_step_forcast)\n",
    "\n",
    "    ARIMA.update(test[i])\n",
    "\n",
    "RMSE_A_H = mean_squared_error(ARIMA_Forecast_H, test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model and forcasted values to a pickel so I dont have to run it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"arima_hour_seasonal\"\n",
    "model_dic = {'model': ARIMA, 'forcasted_values': ARIMA_Forecast_H}\n",
    "\n",
    "with open(str(filename)+'.pkl','wb') as f:\n",
    "    pickle.dump(model_dic,f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternate Fourier Hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_length = 1\n",
    "time_estimate_frames = int(len(test)/time_length)\n",
    "\n",
    "pipe = pipeline.Pipeline([\n",
    "    (\"fourier\", pm.preprocessing.FourierFeaturizer(m=24*7, k = 24)),\n",
    "    (\"arima\", arima.AutoARIMA(stepwise=False, trace=1, error_action=\"ignore\",\n",
    "                              seasonal=False, maxiter=100,\n",
    "                              suppress_warnings=True))])\n",
    "\n",
    "pipe.fit(train)\n",
    "\n",
    "rolling_forecast = []\n",
    "\n",
    "for i in tqdm(range(time_estimate_frames)):\n",
    "\n",
    "    forecast= pipe.predict(n_periods=time_length)\n",
    "    \n",
    "    pipe.update(test[i*time_length:(i+1)*time_length])\n",
    "    \n",
    "    # Append the forecast to the rolling forecast list\n",
    "    rolling_forecast.extend(forecast)\n",
    "\n",
    "RMSE_F_H = mean_squared_error(rolling_forecast, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"fourier_hour_m168_k24_\"\n",
    "model_dic = {'model': pipe, 'forcasted_values': rolling_forecast}\n",
    "\n",
    "with open(str(filename)+'.pkl','wb') as f:\n",
    "    pickle.dump(model_dic,f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Persistence Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persistence_time_length = 24\n",
    "\n",
    "Persistence_day = []\n",
    "\n",
    "time_estimate_frames = int(len(test)/persistence_time_length)\n",
    "\n",
    "for i in range(time_estimate_frames):\n",
    "\n",
    "    spliced_estiamte_frame = data[len(train)+i*persistence_time_length-persistence_time_length:len(train)+(i+1)\n",
    "    *persistence_time_length-persistence_time_length]\n",
    "    \n",
    "    Persistence_day.extend(spliced_estiamte_frame)\n",
    "\n",
    "RMSE_P_D = mean_squared_error(Persistence_day, test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ARIMA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_length = 24\n",
    "\n",
    "ARIMA_Forecast_D = []\n",
    "\n",
    "time_estimate_frames = int(len(test)/time_length)\n",
    "\n",
    "#m is set to 24* due to the seasonal affect of the 24*7 hour energy cycle\n",
    "ARIMA = pm.auto_arima(train, trace = True, seasonal=True, iter = 100)\n",
    "\n",
    "for i in tqdm(range(time_estimate_frames)):\n",
    "    \n",
    "    time_step_forcast = ARIMA.predict(n_periods = time_length)\n",
    "\n",
    "    ARIMA_Forecast_D.extend(time_step_forcast)\n",
    "\n",
    "    ARIMA.update(test[i*time_length:(i+1)*time_length])\n",
    "\n",
    "RMSE_A_D = mean_squared_error(ARIMA_Forecast_D, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"arima_day_seasonal\"\n",
    "model_dic = {'model': ARIMA, 'forcasted_values': ARIMA_Forecast_D}\n",
    "\n",
    "with open(str(filename)+'.pkl','wb') as f:\n",
    "    pickle.dump(model_dic,f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternate Fourier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_length = 24\n",
    "time_estimate_frames = int(len(test)/time_length)\n",
    "\n",
    "pipe = pipeline.Pipeline([\n",
    "    (\"fourier\", pm.preprocessing.FourierFeaturizer(m=24*7, k = 24)),\n",
    "    (\"arima\", arima.AutoARIMA(stepwise=False, trace=1, error_action=\"ignore\",\n",
    "                              seasonal=False, maxiter=100,\n",
    "                              suppress_warnings=True))])\n",
    "\n",
    "pipe.fit(train)\n",
    "\n",
    "rolling_forecast = []\n",
    "\n",
    "for i in tqdm(range(time_estimate_frames)):\n",
    "\n",
    "    forecast= pipe.predict(n_periods=time_length)\n",
    "    \n",
    "    pipe.update(test[i*time_length:(i+1)*time_length])\n",
    "    \n",
    "    # Append the forecast to the rolling forecast list\n",
    "    rolling_forecast.extend(forecast)\n",
    "\n",
    "RMSE_F_D = mean_squared_error(rolling_forecast, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"fourier_day_m168_k24_\"\n",
    "model_dic = {'model': pipe, 'forcasted_values': rolling_forecast}\n",
    "\n",
    "with open(str(filename)+'.pkl','wb') as f:\n",
    "    pickle.dump(model_dic,f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report the RMSE values in all 4 cases (your model\n",
    "and persistence, and for both hour-ahead and day-ahead). Briefly discuss your results and the\n",
    "difference between the two forecasting cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fourier_hour = 'fourier_hour_m168_k24_'\n",
    "#fourier_hour_exog = 'fourier_hour_m168_k24_exog_ExchangeSE_MWh_ OffshoreWindGe100MW_MWh_PowerToHeatMWh'\n",
    "fourier_day = 'fourier_day_m168_k24_'\n",
    "#fourier_day_exog = 'fourier_day_m168_k24_exog_ExchangeSE_MWh_OffshoreWindGe100MW_MWh_PowerToHeatMWhtest'\n",
    "\n",
    "model_list = ['fourier_hour_m168_k24_','fourier_day_m168_k24_']\n",
    "\n",
    "RMSE_list = {}\n",
    "\n",
    "for model in model_list:\n",
    "    with open(model + '.pkl', 'rb') as f:\n",
    "        model_dict = pickle.load(f)\n",
    "\n",
    "    model_info = model_dict['model']\n",
    "    forcasted_values = model_dict['forcasted_values']\n",
    "\n",
    "    RMSE_list[model] = mean_squared_error(forcasted_values, test)\n",
    "\n",
    "RMSE_list['Persistence_Day'] = mean_squared_error(Persistence_day,test)\n",
    "RMSE_list['Persistence_Hour'] = mean_squared_error(Persistence_hour,test)\n",
    "\n",
    "RMSE_list = np.transpose(np.array([list(RMSE_list.keys()),list(RMSE_list.values())]))\n",
    "\n",
    "print(RMSE_list)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2\n",
    "Add any exogenous variables you want (maximum 3) and repeat the process (choose/optimize your\n",
    "model and evaluate it for the hour-ahead and the day-ahead prediction). What exogenous\n",
    "variables helped you improve the prediction and how did you choose the specific ones? Compare\n",
    "your results with those from task 1.1 and briefly discuss them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exog['SpotPriceDKK'] = df_prices['SpotPriceDKK']\n",
    "correlate = df_exog.iloc[:,3:].corr()\n",
    "\n",
    "X_train = X_train[[\"ExchangeSE_MWh\", \"OffshoreWindGe100MW_MWh\",\"PowerToHeatMWh\"]].values\n",
    "X_test = X_test[[\"ExchangeSE_MWh\", \"OffshoreWindGe100MW_MWh\",\"PowerToHeatMWh\"]].values\n",
    "\n",
    "n_X_train = len(X_train)\n",
    "n_X_test = len(X_test)\n",
    "\n",
    "X_train_ar = np.column_stack([np.arange(1,n_X_train+1),X_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "#removing columns\n",
    "df_exog = df_exog.drop(\"HourDK\", axis=1)\n",
    "\n",
    "\n",
    "# plot each column of the DataFrame as a line\n",
    "df_exog.plot(ax=ax)\n",
    "\n",
    "# set the x and y axis labels\n",
    "ax.set_xlabel('Index')\n",
    "ax.set_ylabel('Value')\n",
    "\n",
    "# set the title of the plot\n",
    "ax.set_title('Line Plot of DataFrame')\n",
    "\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_exog = df_exog.drop('PriceArea', axis = 1)\n",
    "record = {}\n",
    "for column in df_exog:\n",
    "    current = np.correlate(df_exog[column], df_prices['SpotPriceDKK'])\n",
    "    record[column] = [current]\n",
    "\n",
    "record = np.transpose(np.array([list(record.keys()),list(record.values())]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hour Ahead Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_length = 1\n",
    "\n",
    "time_estimate_frames = int(len(test)/time_length)\n",
    "\n",
    "pipe = pipeline.Pipeline([\n",
    "    (\"fourier\", pm.preprocessing.FourierFeaturizer(m=24*7, k = 24)),\n",
    "    (\"arima\", arima.AutoARIMA(stepwise=False, trace=1, error_action=\"ignore\",\n",
    "                              seasonal=False, maxiter=100,\n",
    "                              suppress_warnings=True))])\n",
    "\n",
    "pipe.fit(train, X = X_train_ar)\n",
    "\n",
    "rolling_forecast = []\n",
    "\n",
    "for i in tqdm(range(time_estimate_frames)):\n",
    "\n",
    "    X_f = np.column_stack([np.arange(1, time_length+1), X_test[i*time_length:(i+1)*time_length]])\n",
    "\n",
    "    forecast = pipe.predict(n_periods=time_length, X=X_f)\n",
    "    \n",
    "    pipe.update(test[i*time_length:(i+1)*time_length], X = X_f)\n",
    "    \n",
    "    # Append the forecast to the rolling forecast list\n",
    "    rolling_forecast.extend(forecast)\n",
    "\n",
    "#Do I need this line? Not sure\n",
    "rolling_forecast = [0 if x < 0 else x for x in rolling_forecast]\n",
    "\n",
    "RMSE_F_H = mean_squared_error(rolling_forecast, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"fourier_hour_m168_k24_exog_ExchangeSE_MWh_ OffshoreWindGe100MW_MWh_PowerToHeatMWh\"\n",
    "model_dic = {'model': pipe, 'forcasted_values': rolling_forecast}\n",
    "\n",
    "with open(str(filename)+'.pkl','wb') as f:\n",
    "    pickle.dump(model_dic,f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Day Ahead Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_length = 24\n",
    "\n",
    "time_estimate_frames = int(len(test)/time_length)\n",
    "\n",
    "pipe = pipeline.Pipeline([\n",
    "    (\"fourier\", pm.preprocessing.FourierFeaturizer(m=24*7, k = 24)),\n",
    "    (\"arima\", arima.AutoARIMA(stepwise=False, trace=1, error_action=\"ignore\",\n",
    "                              seasonal=False, maxiter=100,\n",
    "                              suppress_warnings=True))])\n",
    "\n",
    "pipe.fit(train, X = X_train_ar)\n",
    "\n",
    "rolling_forecast = []\n",
    "\n",
    "for i in tqdm(range(time_estimate_frames)):\n",
    "\n",
    "    X_f = np.column_stack([np.arange(1, time_length+1), X_test[i*time_length:(i+1)*time_length]])\n",
    "\n",
    "    forecast= pipe.predict(n_periods=time_length, X=X_f)\n",
    "    \n",
    "    pipe.update(test[i*time_length:(i+1)*time_length], X=X_f)\n",
    "    \n",
    "    # Append the forecast to the rolling forecast list\n",
    "    rolling_forecast.extend(forecast)\n",
    "\n",
    "rolling_forecast = [0 if x < 0 else x for x in rolling_forecast]\n",
    "\n",
    "RMSE_F_D = mean_squared_error(rolling_forecast, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"fourier_day_m168_k24_exog_ExchangeSE_MWh_OffshoreWindGe100MW_MWh_PowerToHeatMWh\"\n",
    "model_dic = {'model': pipe, 'forcasted_values': rolling_forecast}\n",
    "\n",
    "with open(str(filename)+'test.pkl','wb') as f:\n",
    "    pickle.dump(model_dic,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fourier_hour = 'fourier_hour_m168_k24_'\n",
    "fourier_hour_exog = 'fourier_hour_m168_k24_exog_ExchangeSE_MWh_ OffshoreWindGe100MW_MWh_PowerToHeatMWh'\n",
    "fourier_day = 'fourier_day_m168_k24_'\n",
    "fourier_day_exog = 'fourier_day_m168_k24_exog_ExchangeSE_MWh_OffshoreWindGe100MW_MWh_PowerToHeatMWhtest'\n",
    "\n",
    "day_model_list = ['fourier_day_m168_k24_','fourier_day_m168_k24_exog_ExchangeSE_MWh_OffshoreWindGe100MW_MWh_PowerToHeatMWhtest']\n",
    "\n",
    "hour_model_list = ['fourier_hour_m168_k24_','fourier_hour_m168_k24_exog_ExchangeSE_MWh_ OffshoreWindGe100MW_MWh_PowerToHeatMWh']\n",
    "\n",
    "RMSE_list = []\n",
    "\n",
    "for model in day_model_list:\n",
    "    with open(model + '.pkl', 'rb') as f:\n",
    "        model_dict = pickle.load(f)\n",
    "\n",
    "    model_info = model_dict['model']\n",
    "    forcasted_values = model_dict['forcasted_values']\n",
    "\n",
    "    RMSE_list.append([\"Day\", model, mean_squared_error(forcasted_values, test)])\n",
    "\n",
    "RMSE_list.append([\"Day\", 'Persistence_Day',mean_squared_error(Persistence_day,test)])\n",
    "\n",
    "for model in hour_model_list:\n",
    "    with open(model + '.pkl', 'rb') as f:\n",
    "        model_dict = pickle.load(f)\n",
    "\n",
    "    model_info = model_dict['model']\n",
    "    forcasted_values = model_dict['forcasted_values']\n",
    "\n",
    "    RMSE_list.append([\"Hour\", model, mean_squared_error(forcasted_values, test)])\n",
    "\n",
    "RMSE_list.append([\"Hour\", 'Persistence_Hour', mean_squared_error(Persistence_hour,test)])\n",
    "\n",
    "print(RMSE_list)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "Develop a temporal Convolutional Neural Network (tCNN) to predict the electricity prices.\n",
    "Your goal in all following tasks is to achieve an as good performance as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new dataframe for all network data and specifiy the features (exog variables & time) and target (SpotPrice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove duplicate label\n",
    "#df_exog = df_exog.drop([\"HourUTC\"], axis=1)\n",
    "\n",
    "df_network = pd.concat([df_prices, df_exog], axis = 1)\n",
    "df_network['Hour'] = df_network['HourUTC'].dt.hour\n",
    "\n",
    "features = [\"ExchangeSE_MWh\",\"OffshoreWindGe100MW_MWh\",\"PowerToHeatMWh\",'Hour']\n",
    "targets = [\"SpotPriceDKK\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into testing and training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = model_selection.train_test_split(df_network, train_size=set_train_size)\n",
    "\n",
    "train_features = train_data[features]\n",
    "test_features = test_data[features]\n",
    "\n",
    "train_labels = train_data[targets] \n",
    "test_labels = test_data[targets]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalise the numerical inputs before applying them to the neural network\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\birdl\\AppData\\Local\\Temp\\ipykernel_28252\\1785373184.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[normalize] = scaler.fit_transform(df[normalize])\n",
      "C:\\Users\\birdl\\AppData\\Local\\Temp\\ipykernel_28252\\1785373184.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[normalize] = scaler.fit_transform(df[normalize])\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "def normalize_df(df):\n",
    "    df.set_index(\"Hour\", inplace=True)\n",
    "\n",
    "    normalize = df.columns[df.columns != \"Hour\"]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    df[normalize] = scaler.fit_transform(df[normalize])\n",
    "\n",
    "    df.reset_index(inplace=True)\n",
    "\n",
    "normalize_df(train_features)\n",
    "normalize_df(test_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_features , val_features , train_labels , val_labels  = train_test_split(train_features, train_labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use inbuilt 'Dataset' class to prepare the data\n",
    "The dataset class is a Pytorch class that characterizes the key features of the dataset you want to generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    'Characterizes a dataset for PyTorch'\n",
    "    def __init__(self, features, labels):\n",
    "            'Initialization'\n",
    "            self.labels = torch.tensor(labels.values) # Target variable\n",
    "            self.features = torch.tensor(features.values, dtype=torch.float) #Input features\n",
    "\n",
    "    def __len__(self):\n",
    "            'Denotes the total number of samples in the dataset'\n",
    "            return len(self.features)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "            'Generates one sample of data'\n",
    "            # Load data and get label\n",
    "            X = self.features[index]\n",
    "            y = self.labels[index]\n",
    "\n",
    "            return X, y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a neural network with pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNModel(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_class):\n",
    "        '''\n",
    "        input_size: specifies the number of features\n",
    "        hidden_size: specifies the number of neurons in the hidden layer\n",
    "        num_class: number of classes for classification\n",
    "        '''\n",
    "        super().__init__()  #initializes the torch.nn.Module\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
    "        self.layer2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.out_layer = nn.Linear(hidden_size, num_class)\n",
    "        self.relu = nn.ReLU() # Activation function\n",
    "        # For more information:https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.relu(self.layer2(x))\n",
    "        x = self.out_layer(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define network hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configurations\n",
    "num_epochs = 100 #Number of epochs\n",
    "\n",
    "# Architecture\n",
    "input_size = len(features)\n",
    "hidden_size = 20\n",
    "num_class = 2\n",
    "\n",
    "lr = 0.01 # Learning rate\n",
    "\n",
    "# Parameters for the data loader\n",
    "params = {'batch_size': 100, # Varry the batch size to find out it's influence accuracy and the rate of convergence\n",
    "          'shuffle': True}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NNModel(input_size, hidden_size, num_class)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Our Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Generators\n",
    "training_set = Dataset(train_features, train_labels)\n",
    "training_generator = torch.utils.data.DataLoader(training_set, **params)\n",
    "\n",
    "val_set = Dataset(val_features, test_labels)\n",
    "val_generator = torch.utils.data.DataLoader(val_set, **params)\n",
    "\n",
    "test_set = Dataset(test_features, test_labels)\n",
    "test_generator = torch.utils.data.DataLoader(test_set, **params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.L1Loss()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizer\n",
    "The optimizer modifies the weights and biases such that the loss is as small as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), # the model parameters being optimized\n",
    "                      lr=lr  # learning rate\n",
    "                     )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to calculate the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(y_pred, y_true):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item() # torch.eq() calculates where two tensors are equal\n",
    "    acc = ((correct/2) / len(y_pred)) * 100 \n",
    "    return acc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Model on Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, training_generator, optimizer, loss_function):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "  \n",
    "    # set model to train mode\n",
    "    model.train()\n",
    "    \n",
    "    for local_batch, local_labels in (training_generator):\n",
    "        \n",
    "        # resets gradient\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass to compute predictions\n",
    "        y_logits = model(local_batch)\n",
    "        #compute the loss\n",
    "        loss = loss_function(y_logits, local_labels.float())\n",
    "        #compute the accuracy\n",
    "        acc = binary_accuracy(y_logits, local_labels)        \n",
    "        \n",
    "        # backpropagation to compute updates model parameters based on loss\n",
    "        loss.backward()\n",
    "        \n",
    "        # update the parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc\n",
    "  \n",
    "    return epoch_loss/len(training_generator), epoch_acc /len(training_generator)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate model on test/validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_generator, loss_function):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "  \n",
    "    # set evaluation mode\n",
    "    model.eval()\n",
    "  \n",
    "    with torch.no_grad():\n",
    "        for local_batch, local_labels in (test_generator):\n",
    "            # compute predictions\n",
    "            y_logits = model(local_batch)\n",
    "            #compute the loss\n",
    "            loss = loss_function(y_logits, local_labels.float())\n",
    "            #compute the accuracy\n",
    "            acc = binary_accuracy(y_logits, local_labels)        \n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc\n",
    "\n",
    "        return epoch_loss/len(test_generator), epoch_acc /len(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\birdl\\anaconda3\\envs\\course02502\\lib\\site-packages\\torch\\nn\\modules\\loss.py:101: UserWarning: Using a target size (torch.Size([100, 1])) that is different to the input size (torch.Size([100, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\birdl\\anaconda3\\envs\\course02502\\lib\\site-packages\\torch\\nn\\modules\\loss.py:101: UserWarning: Using a target size (torch.Size([67, 1])) that is different to the input size (torch.Size([67, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\birdl\\anaconda3\\envs\\course02502\\lib\\site-packages\\torch\\nn\\modules\\loss.py:101: UserWarning: Using a target size (torch.Size([44, 1])) that is different to the input size (torch.Size([44, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\birdl\\anaconda3\\envs\\course02502\\lib\\site-packages\\torch\\nn\\modules\\loss.py:101: UserWarning: Using a target size (torch.Size([42, 1])) that is different to the input size (torch.Size([42, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 11, train loss: 312.0202, train accuracy: 0.00%\n",
      "epoch: 11, validation loss: 638.7987, validation accuracy: 0.00%\n",
      "epoch: 21, train loss: 277.7044, train accuracy: 0.00%\n",
      "epoch: 21, validation loss: 575.5252, validation accuracy: 0.00%\n",
      "epoch: 31, train loss: 274.4981, train accuracy: 0.00%\n",
      "epoch: 31, validation loss: 586.4621, validation accuracy: 0.00%\n",
      "epoch: 41, train loss: 272.1250, train accuracy: 0.00%\n",
      "epoch: 41, validation loss: 584.6393, validation accuracy: 0.00%\n",
      "epoch: 51, train loss: 273.1511, train accuracy: 0.00%\n",
      "epoch: 51, validation loss: 595.0097, validation accuracy: 0.00%\n",
      "epoch: 61, train loss: 269.2196, train accuracy: 0.00%\n",
      "epoch: 61, validation loss: 585.8925, validation accuracy: 0.00%\n",
      "epoch: 71, train loss: 270.0388, train accuracy: 0.00%\n",
      "epoch: 71, validation loss: 593.8732, validation accuracy: 0.00%\n",
      "epoch: 81, train loss: 268.8330, train accuracy: 0.00%\n",
      "epoch: 81, validation loss: 579.6116, validation accuracy: 0.00%\n",
      "epoch: 91, train loss: 268.9470, train accuracy: 0.00%\n",
      "epoch: 91, validation loss: 579.2346, validation accuracy: 0.00%\n",
      "epoch: 101, train loss: 269.0997, train accuracy: 0.00%\n",
      "epoch: 101, validation loss: 609.4805, validation accuracy: 0.00%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def predict_consumption(model):\n",
    "    \n",
    "    train_acc = []\n",
    "    test_acc = []\n",
    "    val_acc = []\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(1, 1 + num_epochs):\n",
    "\n",
    "        train_loss, train_accuracy = train_model(model, training_generator, optimizer, loss_function)\n",
    "        \n",
    "        #Add code to evaluate your model on the test database\n",
    "        epoch_loss, epoch_acc = evaluate_model(model, test_generator, loss_function)\n",
    "\n",
    "        validation_loss, validation_accuracy = evaluate_model(model, val_generator, loss_function)\n",
    "                \n",
    "        # Add code to save the acccuracy at each epoch\n",
    "        train_acc.append(train_accuracy)\n",
    "        test_acc.append(epoch_acc)\n",
    "        val_acc.append(validation_accuracy)\n",
    "        \n",
    "        #Save model with the best validation loss\n",
    "        if validation_loss < best_val_loss:\n",
    "            best_val_loss = validation_loss\n",
    "            torch.save(model, 'model.pth')\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'epoch: {epoch+1}, train loss: {train_loss:.4f}, train accuracy: {train_accuracy:.2f}%')\n",
    "            print(f'epoch: {epoch+1}, validation loss: {validation_loss:.4f}, validation accuracy: {validation_accuracy:.2f}%')\n",
    "        \n",
    "    #Add code to plot the train and test accuracy against the epochs\n",
    "    \n",
    "    plt.plot(test_acc)\n",
    "    plt.plot(train_acc)\n",
    "predict_consumption(model)\n",
    "               "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.1\n",
    "\n",
    "Pick a maximum of 3 exogenous variables and perform the hour-ahead prediction for the\n",
    "training dataset. Report the RMSE value you achieve with your model and compare with the\n",
    "persistence RMSE you established in Task 1."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "course02502",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
